import gradio as gr
import pandas as pd
import google.generativeai as genai
import os
from dotenv import load_dotenv
from pathlib import Path
import traceback
import json
import torch
import torch.nn.functional as F
import numpy as np  
# Import c√°c h√†m t·ª´ Giai ƒëo·∫°n 0 (c·∫ßn file utils.py)
from utils import get_clip_text_features, load_all_models # Ch·ªâ c·∫ßn 2 h√†m n√†y

# --- 1. C·∫§U H√åNH & KH·ªûI ƒê·ªòNG ---
print("--- KH·ªûI ƒê·ªòNG BU·ªíNG L√ÅI AI T√ÅC CHI·∫æN (PHI√äN B·∫¢N THI ƒê·∫§U V4) ---")

# T·∫£i key v√† k·∫øt n·ªëi t·ªõi Gemini
load_dotenv(); genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel('gemini-pro')
print("‚úÖ K·∫øt n·ªëi t·ªõi Gemini API th√†nh c√¥ng.")

# T·∫£i CSDL
DB_PATH = Path('./god_database_final.parquet')
df = pd.read_parquet(DB_PATH)
# Chuy·ªÉn ƒë·ªïi feature t·ª´ list v·ªÅ numpy array ƒë·ªÉ t√≠nh to√°n
df['clip_feature'] = df['clip_feature'].apply(np.array)
print(f"‚úÖ 'Si√™u CSDL' v√† Kho d·ª± ph√≤ng CLIP ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")

# T·∫£i m·ªôt ph·∫ßn c·ªßa model CLIP ƒë·ªÉ encode text query (ch·ªâ c·∫ßn model v√† processor)
MODELS_DIR = Path('./models/')
try:
    _, clip_model, clip_proc, _ = load_all_models(
        'yolov8x.pt', # T√™n placeholder, v√¨ load_yolo=False
        "openai/clip-vit-base-patch32",
        "vit_h",      # T√™n placeholder, v√¨ load_sam=False
        MODELS_DIR / "sam_vit_h_4b8939.pth",
        'cpu',
        models_dir=MODELS_DIR, # Th√™m tham s·ªë b·∫Øt bu·ªôc
        load_yolo=False,
        load_sam=False
    )
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    clip_model.to(device)
    print(f"‚úÖ Model CLIP ƒë√£ s·∫µn s√†ng cho suy lu·∫≠n ng·ªØ nghƒ©a tr√™n thi·∫øt b·ªã {device}.")
except Exception as e:
    print(f"‚ùå L·ªñI khi t·∫£i model CLIP: {e}")
    clip_model, clip_proc = None, None
device = 'cuda' if torch.cuda.is_available() else 'cpu'
clip_model.to(device)
print(f"‚úÖ Model CLIP ƒë√£ s·∫µn s√†ng cho suy lu·∫≠n ng·ªØ nghƒ©a tr√™n thi·∫øt b·ªã {device}.")

# [C·∫¢I TI·∫æN Gƒê3] Qu·∫£n l√Ω tr·∫°ng th√°i v√† file submission
SUBMISSION_PATH = Path('./AI25-15.json') # T√™n file n·ªôp b√†i
final_submission = {} # Bi·∫øn state trong b·ªô nh·ªõ

def load_submission_state():
    """T·∫£i tr·∫°ng th√°i t·ª´ file JSON n·∫øu t·ªìn t·∫°i."""
    global final_submission
    if SUBMISSION_PATH.exists():
        try:
            with open(SUBMISSION_PATH, 'r', encoding='utf8') as f:
                final_submission = json.load(f)
            print(f"‚úÖ ƒê√£ t·∫£i l·∫°i tr·∫°ng th√°i submission t·ª´ file '{SUBMISSION_PATH}'.")
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: File '{SUBMISSION_PATH}' b·ªã l·ªói, b·∫Øt ƒë·∫ßu v·ªõi submission r·ªóng.")
            final_submission = {}
    else:
        print("‚úÖ Kh·ªüi t·∫°o submission m·ªõi.")

def save_submission_state():
    """L∆∞u tr·∫°ng th√°i hi·ªán t·∫°i v√†o file JSON."""
    global final_submission
    try:
        with open(SUBMISSION_PATH, 'w', encoding='utf8') as f:
            json.dump(final_submission, f, indent=4)
        return True
    except Exception as e:
        print(f"‚ùå L·ªñI khi l∆∞u file submission: {e}")
        return False

# T·∫£i tr·∫°ng th√°i l·∫ßn ƒë·∫ßu khi kh·ªüi ƒë·ªông
load_submission_state()


# --- 2. PROMPT V4 (Gi·ªØ nguy√™n) ---
df_info_str = ""
if df is not None:
    df_info_str = f"""- DataFrame `df` c√≥ {len(df)} d√≤ng.
- C√°c c·ªôt ch√≠nh bao g·ªìm: {list(df.columns)}
- V√≠ d·ª• 2 d√≤ng ƒë·∫ßu ti√™n:\n{df.head(2).to_string()}"""

MASTER_PROMPT_TEMPLATE = """
B·∫†N L√Ä M·ªòT ENGINE D·ªäCH NG√îN NG·ªÆ T·ª∞ NHI√äN SANG CODE PANDAS C·ª∞C K·ª≤ CH√çNH X√ÅC.
M·ª•c ti√™u c·ªßa b·∫°n l√† d·ªãch y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt kh·ªëi code Pandas duy nh·∫•t.

## B·ªêI C·∫¢NH M√îI TR∆Ø·ªúNG TH·ª∞C THI:
{df_info}

## QUY TR√åNH L√ÄM VI·ªÜC B·∫ÆT BU·ªòC:
1.  **KH√îNG** t·ª± t·∫°o DataFrame m·∫´u. DataFrame `df` ƒë√£ t·ªìn t·∫°i trong m√¥i tr∆∞·ªùng th·ª±c thi.
2.  Ph√¢n t√≠ch y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ t·∫°o ra m·ªôt DataFrame cu·ªëi c√πng c√≥ t√™n `filtered_df`. `filtered_df` ch·ªâ n√™n ch·ª©a c√°c d√≤ng th·ªèa m√£n T·∫§T C·∫¢ c√°c ƒëi·ªÅu ki·ªán.
3.  **S·ª¨ D·ª§NG KHU√îN M·∫™U SAU ƒê·ªÇ T·∫†O K·∫æT QU·∫¢ CU·ªêI C√ôNG:** Sau khi ƒë√£ c√≥ `filtered_df`, h√£y k·∫øt th√∫c ƒëo·∫°n code c·ªßa b·∫°n b·∫±ng **CH√çNH X√ÅC** 3 d√≤ng sau. ƒê·ª´ng thay ƒë·ªïi ch√∫ng.

    ```python
    # --- KHU√îN M·∫™U K·∫æT QU·∫¢ (KH√îNG THAY ƒê·ªîI) ---
    if filtered_df.empty:
        q_result = {{}}
    else:
        # [S·ª¨A L·ªñI] Th√™m b∆∞·ªõc √©p ki·ªÉu `int()` ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi JSON
        q_result = filtered_df.groupby('video_name')['frame_id'].apply(lambda x: sorted([int(i) for i in x.unique()])).to_dict()
    ```

## Y√äU C·∫¶U TUY·ªÜT ƒê·ªêI:
- **KH√îNG** ƒë·ªãnh nghƒ©a h√†m m·ªõi.
- **KH√îNG** s·ª≠ d·ª•ng `re` (regular expressions) ho·∫∑c `eval()`.
- **KH√îNG** tham chi·∫øu ƒë·∫øn bi·∫øn `user_query`.
- To√†n b·ªô logic c·ªßa b·∫°n ph·∫£i d·∫´n ƒë·∫øn vi·ªác t·∫°o ra m·ªôt DataFrame t√™n l√† `filtered_df` ƒë·ªÉ ƒë∆∞a v√†o Khu√¥n M·∫´u K·∫øt Qu·∫£.

---
Y√äU C·∫¶U C·ª¶A NG∆Ø·ªúI D√ôNG:
{user_query}
"""

# --- 3. H√ÄM L√ïI T∆Ø∆†NG T√ÅC (Gi·ªØ nguy√™n logic, th√™m c·∫≠p nh·∫≠t state) ---
def generate_code_from_query(question_id: str, user_query: str):
    # H√†m n√†y gi·ªØ nguy√™n nh∆∞ Gƒê2, ch·ªâ t·∫°o code v√† kh√¥ng thay ƒë·ªïi state
    if not model or df is None or not question_id or not user_query:
        error_msg = "L·ªñI: Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi API, file CSDL v√† nh·∫≠p ƒë·ªß M√£ c√¢u h·ªèi & Y√™u c·∫ßu."
        return "# L·ªói ƒë·∫ßu v√†o", error_msg, final_submission
    status_log = f"‚ñ∂Ô∏è ƒêang t·∫°o l·ªánh cho c√¢u h·ªèi [{question_id}]...\n"
    status_log += "  -> ƒêang g·ª≠i y√™u c·∫ßu t·ªõi Gemini API...\n"
    try:
        request_options = {"timeout": 30}
        response = model.generate_content(MASTER_PROMPT_TEMPLATE.format(df_info=df_info_str, user_query=user_query), request_options=request_options)
        generated_code = response.text.replace("```python", "").replace("```", "").strip()
        status_log += "  -> ƒê√£ nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi. Code ƒë√£ s·∫µn s√†ng.\n"
    except Exception as e:
        status_log += f"‚ùå L·ªñI KHI G·ªåI GEMINI API: {e}\n"
        generated_code = f"# L·ªói API: {e}"
    return generated_code, status_log, final_submission

def handle_semantic_query(user_query, similarity_threshold=0.25):
    status_log = "‚ñ∂Ô∏è Ch·∫ø ƒë·ªô Suy lu·∫≠n Ng·ªØ nghƒ©a (CLIP) ƒë∆∞·ª£c k√≠ch ho·∫°t.\n"
    try:
        # 1. Encode text query
        status_log += f"  -> ƒêang encode y√™u c·∫ßu: '{user_query}'...\n"
        text_feature = get_clip_text_features([user_query], clip_model, clip_proc, device).numpy()

        # 2. T√≠nh to√°n cosine similarity tr·ª±c ti·∫øp tr√™n c·ªôt 'clip_feature'
        status_log += f"  -> ƒêang so s√°nh v·ªõi {len(df)} vector ·∫£nh...\n"
        image_features_stack = np.stack(df['clip_feature'].values)
        
        text_tensor = torch.from_numpy(text_feature).to(device)
        image_tensor = torch.from_numpy(image_features_stack).to(device)
        
        similarities = F.cosine_similarity(text_tensor, image_tensor).cpu().numpy()
        
        # 3. L·ªçc tr·ª±c ti·∫øp DataFrame ch√≠nh
        matched_df = df[similarities >= similarity_threshold]
        status_log += f"  -> T√¨m th·∫•y {len(matched_df)} ƒë·ªëi t∆∞·ª£ng ti·ªÅm nƒÉng.\n"

        # 4. G√°n `filtered_df` v√† d√πng khu√¥n m·∫´u ƒë·ªÉ t·∫°o k·∫øt qu·∫£
        # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ t√°i s·ª≠ d·ª•ng logic t·∫°o q_result
        filtered_df = matched_df
        
        if filtered_df.empty:
            q_result = {}
        else:
            q_result = filtered_df.groupby('video_name')['frame_id'].apply(lambda x: sorted([int(i) for i in x.unique()])).to_dict()
        
        status_log += "‚úÖ Suy lu·∫≠n ng·ªØ nghƒ©a ho√†n t·∫•t.\n"
        return q_result, status_log

    except Exception as e:
        tb_str = traceback.format_exc()
        status_log += f"‚ùå L·ªñI trong qu√° tr√¨nh suy lu·∫≠n ng·ªØ nghƒ©a:\n{tb_str}\n"
        return None, status_log


def execute_code(question_id, manual_code, use_clip_mode, user_query):
    global final_submission
    if df is None or not question_id:
        return "L·ªñI: Thi·∫øu th√¥ng tin ƒë·∫ßu v√†o.", final_submission

    q_result = None
    status_log = ""

    if use_clip_mode:
        # Ch·∫ø ƒë·ªô suy lu·∫≠n ng·ªØ nghƒ©a n√¢ng cao
        if not user_query:
            status_log = "L·ªñI: Ch·∫ø ƒë·ªô Suy lu·∫≠n Ng·ªØ nghƒ©a c·∫ßn c√≥ 'Y√™u c·∫ßu truy v·∫•n' ƒë·ªÉ ho·∫°t ƒë·ªông."
            return status_log, final_submission
        q_result, status_log = handle_semantic_query(user_query)
    else:
        # Ch·∫ø ƒë·ªô sinh code Pandas th√¥ng th∆∞·ªùng
        if not manual_code:
            status_log = "L·ªñI: Kh√¥ng c√≥ code ƒë·ªÉ th·ª±c thi."
            return status_log, final_submission
        status_log = f"‚ñ∂Ô∏è ƒêang th·ª±c thi code Pandas cho c√¢u h·ªèi [{question_id}]...\n"
        try:
            execution_scope = {'df': df}
            exec(manual_code, globals(), execution_scope)
            q_result = execution_scope.get('q_result')
            if q_result is None: raise ValueError("Kh√¥ng t√¨m th·∫•y bi·∫øn 'q_result'.")
        except Exception:
            tb_str = traceback.format_exc()
            status_log += f"‚ùå L·ªñI KHI TH·ª∞C THI CODE:\n{tb_str}\n"
            return status_log, final_submission
            
    # C·∫≠p nh·∫≠t state v√† l∆∞u file n·∫øu c√≥ k·∫øt qu·∫£ h·ª£p l·ªá
    if q_result is not None:
        final_submission[question_id] = q_result
        if save_submission_state():
            status_log += f"‚úÖ TH√ÄNH C√îNG! K·∫øt qu·∫£ cho c√¢u h·ªèi [{question_id}] ƒë√£ ƒë∆∞·ª£c C·∫¨P NH·∫¨T v√† L∆ØU.\n"
        else:
            status_log += f"‚ö†Ô∏è L·ªñI: Kh√¥ng th·ªÉ l∆∞u file submission!\n"
    
    return status_log, final_submission

# --- 4. GIAO DI·ªÜN GRADIO (PHI√äN B·∫¢N THI ƒê·∫§U) ---
with gr.Blocks(theme=gr.themes.Monochrome(), title="Bu·ªìng l√°i AI") as app:
    gr.Markdown("# üöÄ BU·ªíNG L√ÅI AI T√ÅC CHI·∫æN (V4 - THI ƒê·∫§U) üöÄ")
    
    with gr.Row():
        with gr.Column(scale=2):
            gr.Markdown("### 1. Ra L·ªánh")
            q_id_input = gr.Textbox(label="M√£ c√¢u h·ªèi", placeholder="V√≠ d·ª•: 1")
            user_query_input = gr.Textbox(label="Y√™u c·∫ßu truy v·∫•n", placeholder="V√≠ d·ª•: t√¨m xe bus m√†u x√°m", lines=3)
            # [C·∫¢I TI·∫æN Gƒê4] Checkbox k√≠ch ho·∫°t ch·∫ø ƒë·ªô CLIP
            clip_mode_checkbox = gr.Checkbox(label="S·ª≠ d·ª•ng Suy lu·∫≠n Ng·ªØ nghƒ©a (CLIP - Ch·∫≠m)", info="D√πng cho c√°c c√¢u h·ªèi ph·ª©c t·∫°p v·ªÅ h√†nh ƒë·ªông, thu·ªôc t√≠nh kh√¥ng c√≥ s·∫µn.")
            with gr.Row():
                # [C·∫¢I TI·∫æN Gƒê4] N√∫t Clear
                clear_button = gr.ClearButton(value="X√≥a L·ªánh")
                generate_button = gr.Button("T·∫°o L·ªánh (AI)", variant="secondary")
        
        with gr.Column(scale=3):
            gr.Markdown("### 3. Nh·∫≠t K√Ω & Tr·∫°ng Th√°i")
            status_log_output = gr.Textbox(label="Nh·∫≠t k√Ω h·ªá th·ªëng", lines=8, interactive=False)

    gr.Markdown("### 2. Tinh Ch·ªânh & Thi H√†nh")
    manual_code_input = gr.Code(label="B√†n l√†m vi·ªác: Code do AI t·∫°o (s·ª≠a n·∫øu c·∫ßn)", language="python", interactive=True)
    execute_button = gr.Button("Thi H√†nh & L∆∞u K·∫øt Qu·∫£", variant="primary")
    
    gr.Markdown("---")
    gr.Markdown("### 4. K·∫øt Qu·∫£ B√†i L√†m Hi·ªán T·∫°i")
    submission_output = gr.JSON(value=final_submission, label=f"N·ªôi dung file: {SUBMISSION_PATH}")

    # K·∫øt n·ªëi s·ª± ki·ªán
    generate_button.click(fn=generate_code_from_query, inputs=[q_id_input, user_query_input], outputs=[manual_code_input, status_log_output, submission_output])
    execute_button.click(fn=execute_code, inputs=[q_id_input, manual_code_input, clip_mode_checkbox, user_query_input], outputs=[status_log_output, submission_output])
    clear_button.add([q_id_input, user_query_input, manual_code_input])

if __name__ == "__main__":
    app.launch()